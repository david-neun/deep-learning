{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fnyj-l2vzzTk",
        "outputId": "4254f758-a035-40a4-ae6a-b85deaa0ff6a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating new Ultralytics Settings v0.0.6 file ✅ \n",
            "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
            "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n",
            "Mounted at /content/drive\n",
            "\u001b[KDownloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8n.pt to 'yolov8n.pt': 100% ━━━━━━━━━━━━ 6.2MB 104.0MB/s 0.1s\n",
            "t = 0.00s, signal = red, train = not_present\n",
            "t = 1.90s, signal = red, train = just_entered\n",
            "t = 1.95s, signal = red, train = moving\n",
            "t = 6.00s, signal = red, train = left_the_frame\n",
            "t = 6.02s, signal = red, train = not_present\n"
          ]
        }
      ],
      "source": [
        "# CSC 760 - Deep Learning - Fall 2025 - SD School of Mines And Tech\n",
        "#\n",
        "# Professor:\n",
        "#    Dr. Nirmalya Thakur\n",
        "# TA:\n",
        "#    PhD Student Faria Nishat Khan\n",
        "#\n",
        "# Project 2 (Submission Nov 7, 2025)\n",
        "#\n",
        "# Description: This script implements a computer vision system that:\n",
        "# 1. Detects traffic signals and trains using YOLO\n",
        "# 2. Classifies traffic signal colors (red, yellow, green)\n",
        "# 3. Tracks train states through a state machine\n",
        "# 4. Outputs timestamped state changes\n",
        "#\n",
        "# Team Members: Luke Videckis and Jose David Cortes\n",
        "\n",
        "\n",
        "# **** Libraries\n",
        "# TODO\n",
        "# we should clean and remove the libraries that we are not using on this project\n",
        "import sys, subprocess, importlib, os\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "# ----------------------------\n",
        "# Lightweight dependency setup\n",
        "# ----------------------------\n",
        "def _ensure(pkg):\n",
        "    name = pkg.split(\"==\")[0].split(\">=\")[0].split(\"[\")[0]\n",
        "    try:\n",
        "        importlib.import_module(name)\n",
        "    except ImportError:\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", pkg, \"--quiet\"])\n",
        "\n",
        "# Public packages only (per Project2.pdf notes)\n",
        "for p in [\"ultralytics>=8.2.0\", \"numpy\", \"opencv-python\"]:\n",
        "    _ensure(p)\n",
        "\n",
        "from ultralytics import YOLO\n",
        "\n",
        "# ----------------------------\n",
        "# Configuration parameters\n",
        "# ----------------------------\n",
        "class Cfg:\n",
        "    # Model and detection\n",
        "    yolo_weights = \"yolov8n.pt\"   # COCO model; includes 'traffic light' & 'train' classes\n",
        "    yolo_conf = 0.30\n",
        "    yolo_iou  = 0.50\n",
        "\n",
        "    # Train-state logic\n",
        "    min_train_area_px = 8000      # ignore tiny train boxes\n",
        "    miss_patience = 6             # frames without train before \"left_the_frame\"\n",
        "    just_entered_grace = 3        # frames allowed to settle after first appearance\n",
        "    orb_nfeatures = 1000\n",
        "    motion_speed_thresh = 1.0     # px/frame; median ORB displacement\n",
        "    xor_dx_thresh = 1.0           # px; fallback horizontal shift\n",
        "\n",
        "    # Signal color (BGR-domain, as required)\n",
        "    red_min   = 140\n",
        "    green_min = 140\n",
        "    blue_max_for_yellow = 120\n",
        "    rg_margin = 20\n",
        "\n",
        "    # Output/visualization\n",
        "    annotate_video = True\n",
        "    out_video_path = \"annotated_output.mp4\"\n",
        "    draw_every_n = 1\n",
        "\n",
        "cfg = Cfg()\n",
        "\n",
        "# ----------------------------\n",
        "# YOLO detector wrapper\n",
        "# ----------------------------\n",
        "class Detector:\n",
        "    def __init__(self, weights):\n",
        "        self.model = YOLO(weights)\n",
        "        names = self.model.model.names if hasattr(self.model, \"model\") else self.model.names\n",
        "        self.names = names\n",
        "        self.cls_traffic = None\n",
        "        self.cls_train = None\n",
        "        for k,v in names.items():\n",
        "            n = str(v).lower()\n",
        "            if n == \"traffic light\":\n",
        "                self.cls_traffic = k\n",
        "            if n == \"train\":\n",
        "                self.cls_train = k\n",
        "        if self.cls_traffic is None or self.cls_train is None:\n",
        "            raise RuntimeError(\"Required classes not found in model: 'traffic light' and 'train'.\")\n",
        "\n",
        "    def infer(self, frame_bgr):\n",
        "        res = self.model.predict(\n",
        "            source=frame_bgr,\n",
        "            conf=cfg.yolo_conf,\n",
        "            iou=cfg.yolo_iou,\n",
        "            classes=[self.cls_traffic, self.cls_train],\n",
        "            verbose=False,\n",
        "            device=\"cpu\"\n",
        "        )[0]\n",
        "        tl_boxes, tr_boxes = [], []\n",
        "        if res and res.boxes is not None:\n",
        "            for b in res.boxes:\n",
        "                cls_id = int(b.cls.item())\n",
        "                box = b.xyxy.cpu().numpy().astype(int).ravel().tolist()\n",
        "                conf = float(b.conf.item())\n",
        "                if cls_id == self.cls_traffic:\n",
        "                    tl_boxes.append((box, conf))\n",
        "                elif cls_id == self.cls_train:\n",
        "                    tr_boxes.append((box, conf))\n",
        "        return tl_boxes, tr_boxes\n",
        "\n",
        "# ----------------------------\n",
        "# BGR-based signal color\n",
        "# ----------------------------\n",
        "def classify_signal_color_bgr(roi_bgr, prev_color=None):\n",
        "    if roi_bgr is None or roi_bgr.size == 0:\n",
        "        return prev_color or \"unknown\"\n",
        "    B,G,R = cv2.split(roi_bgr)\n",
        "    red_mask   = (R > cfg.red_min)   & (R > G + cfg.rg_margin) & (R > B + cfg.rg_margin)\n",
        "    green_mask = (G > cfg.green_min) & (G > R + cfg.rg_margin) & (G > B + cfg.rg_margin)\n",
        "    yellow_mask = (R > cfg.red_min) & (G > cfg.green_min) & (B < cfg.blue_max_for_yellow)\n",
        "\n",
        "    def vote(mask, channel):\n",
        "        cnt = float(mask.sum())\n",
        "        return 0.0 if cnt <= 0 else float(channel[mask].mean()) * (cnt / mask.size)\n",
        "\n",
        "    s_red    = vote(red_mask, R)\n",
        "    s_green  = vote(green_mask, G)\n",
        "    s_yellow = vote(yellow_mask, (R.astype(np.float32)+G.astype(np.float32))/2.0)\n",
        "\n",
        "    scores = {\"red\": s_red, \"green\": s_green, \"yellow\": s_yellow}\n",
        "    color = max(scores.items(), key=lambda kv: kv[1])[0]\n",
        "    if scores[color] < 1.0:\n",
        "        return prev_color or \"unknown\"\n",
        "    return color\n",
        "\n",
        "# ----------------------------\n",
        "# ORB motion + XOR fallback\n",
        "# ----------------------------\n",
        "def orb_speed(prev_gray, curr_gray, roi):\n",
        "    x1,y1,x2,y2 = roi\n",
        "    h,w = prev_gray.shape\n",
        "    x1,y1 = max(0,x1), max(0,y1)\n",
        "    x2,y2 = min(w,x2), min(h,y2)\n",
        "    if x2<=x1 or y2<=y1:\n",
        "        return 0.0, 0\n",
        "    g1 = prev_gray[y1:y2, x1:x2]\n",
        "    g2 = curr_gray[y1:y2, x1:x2]\n",
        "    if g1.size==0 or g2.size==0:\n",
        "        return 0.0, 0\n",
        "\n",
        "    diff = cv2.absdiff(g2, g1)\n",
        "    _, mask = cv2.threshold(diff, 25, 255, cv2.THRESH_BINARY)\n",
        "    orb = cv2.ORB_create(nfeatures=cfg.orb_nfeatures)\n",
        "    k1,d1 = orb.detectAndCompute(g1, mask)\n",
        "    k2,d2 = orb.detectAndCompute(g2, mask)\n",
        "    if d1 is None or d2 is None or len(d1)==0 or len(d2)==0:\n",
        "        return 0.0, 0\n",
        "    bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
        "    matches = sorted(bf.match(d1, d2), key=lambda m:m.distance)[:80]\n",
        "    if not matches:\n",
        "        return 0.0, 0\n",
        "    P1 = np.float32([k1[m.queryIdx].pt for m in matches])\n",
        "    P2 = np.float32([k2[m.trainIdx].pt for m in matches])\n",
        "    speed = float(np.median(np.linalg.norm(P2-P1, axis=1)))\n",
        "    return speed, len(matches)\n",
        "\n",
        "def xor_dx(prev_gray, curr_gray, roi):\n",
        "    x1,y1,x2,y2 = roi\n",
        "    h,w = prev_gray.shape\n",
        "    x1,y1 = max(0,x1), max(0,y1)\n",
        "    x2,y2 = min(w,x2), min(h,y2)\n",
        "    if x2<=x1 or y2<=y1:\n",
        "        return 0.0\n",
        "    g1 = prev_gray[y1:y2, x1:x2]\n",
        "    g2 = curr_gray[y1:y2, x1:x2]\n",
        "    E1 = cv2.Canny(g1, 100, 200)\n",
        "    E2 = cv2.Canny(g2, 100, 200)\n",
        "    XOR = cv2.bitwise_xor(E1, E2)\n",
        "    E1m = cv2.bitwise_and(E1, XOR)\n",
        "    E2m = cv2.bitwise_and(E2, XOR)\n",
        "    p1 = E1m.sum(axis=0).astype(np.float32)\n",
        "    p2 = E2m.sum(axis=0).astype(np.float32)\n",
        "    if np.all(p1==0) or np.all(p2==0):\n",
        "        return 0.0\n",
        "    c = np.correlate(p2 - p2.mean(), p1 - p1.mean(), mode='full')\n",
        "    shift = int(np.argmax(c) - (len(p1) - 1))\n",
        "    return float(shift)\n",
        "\n",
        "# ----------------------------\n",
        "# Train state machine\n",
        "# ----------------------------\n",
        "from dataclasses import dataclass\n",
        "\n",
        "@dataclass\n",
        "class TrainState:\n",
        "    mode: str = \"not_present\"      # not_present | just_entered | moving | left_the_frame\n",
        "    miss_count: int = 0\n",
        "    just_counter: int = 0\n",
        "    had_presence: bool = False\n",
        "    last_roi: tuple = (0,0,0,0)\n",
        "\n",
        "def pick_largest_valid_train(tr_boxes):\n",
        "    best, area = None, 0\n",
        "    for (x1,y1,x2,y2), conf in tr_boxes:\n",
        "        w,h = max(0,x2-x1), max(0,y2-y1)\n",
        "        a = w*h\n",
        "        if a > area:\n",
        "            best, area = (x1,y1,x2,y2), a\n",
        "    if area < cfg.min_train_area_px:\n",
        "        return None\n",
        "    return best\n",
        "\n",
        "def update_train_state(ts: TrainState, prev_gray, curr_gray, roi):\n",
        "    present = roi is not None\n",
        "    if not present:\n",
        "        ts.miss_count += 1\n",
        "        if ts.had_presence and ts.miss_count >= cfg.miss_patience and ts.mode != \"left_the_frame\":\n",
        "            ts.mode = \"left_the_frame\"\n",
        "            ts.had_presence = False\n",
        "            ts.just_counter = 0\n",
        "        elif not ts.had_presence:\n",
        "            ts.mode = \"not_present\"\n",
        "        return ts\n",
        "\n",
        "    ts.miss_count = 0\n",
        "    if not ts.had_presence:\n",
        "        ts.mode = \"just_entered\"\n",
        "        ts.had_presence = True\n",
        "        ts.just_counter = 0\n",
        "    else:\n",
        "        speed, nm = orb_speed(prev_gray, curr_gray, roi)\n",
        "        if speed < cfg.motion_speed_thresh:\n",
        "            dx = xor_dx(prev_gray, curr_gray, roi)\n",
        "            if abs(dx) >= cfg.xor_dx_thresh:\n",
        "                speed = abs(dx)\n",
        "        if ts.mode == \"just_entered\":\n",
        "            ts.just_counter += 1\n",
        "            if ts.just_counter >= cfg.just_entered_grace or speed >= cfg.motion_speed_thresh:\n",
        "                ts.mode = \"moving\"\n",
        "        else:\n",
        "            if speed >= cfg.motion_speed_thresh:\n",
        "                ts.mode = \"moving\"\n",
        "    ts.last_roi = roi\n",
        "    return ts\n",
        "\n",
        "def format_event(t_s, signal_color, train_mode):\n",
        "    return f\"t = {t_s:.2f}s, signal = {signal_color}, train = {train_mode}\"\n",
        "\n",
        "# ----------------------------\n",
        "# Main pipeline\n",
        "# ----------------------------\n",
        "def run_pipeline(video_path, write_video=cfg.annotate_video):\n",
        "    det = Detector(cfg.yolo_weights)\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    if not cap.isOpened():\n",
        "        raise SystemExit(f\"Cannot open video: {video_path}\")\n",
        "\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS) or 30.0\n",
        "    W  = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH) or 640)\n",
        "    H  = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT) or 360)\n",
        "\n",
        "    writer = None\n",
        "    if write_video:\n",
        "        fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
        "        writer = cv2.VideoWriter(cfg.out_video_path, fourcc, fps, (W, H))\n",
        "\n",
        "    ok, frame0 = cap.read()\n",
        "    if not ok:\n",
        "        raise SystemExit(\"Empty video.\")\n",
        "    gray_prev = cv2.cvtColor(frame0, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Process first frame at t=0\n",
        "    tl_boxes, tr_boxes = det.infer(frame0)\n",
        "    tl_roi = None\n",
        "    if tl_boxes:\n",
        "        (x1,y1,x2,y2), _ = sorted(tl_boxes, key=lambda x:x[1], reverse=True)[0]\n",
        "        x1,y1 = max(0,x1), max(0,y1)\n",
        "        x2,y2 = min(W-1,x2), min(H-1,y2)\n",
        "        if x2>x1 and y2>y1:\n",
        "            tl_roi = frame0[y1:y2, x1:x2]\n",
        "    last_signal = classify_signal_color_bgr(tl_roi, prev_color=None)\n",
        "    ts = TrainState()\n",
        "    train_roi = pick_largest_valid_train(tr_boxes)\n",
        "    ts = update_train_state(ts, gray_prev, gray_prev, train_roi)  # prev==curr at t=0\n",
        "    last_reported_signal = None\n",
        "    last_reported_train  = None\n",
        "\n",
        "    # Emit initial line\n",
        "    line0 = format_event(0.0, last_signal, ts.mode)\n",
        "    print(line0)\n",
        "    last_reported_signal = last_signal\n",
        "    last_reported_train  = ts.mode\n",
        "\n",
        "    # Annotate first frame\n",
        "    if writer is not None:\n",
        "        draw = frame0.copy()\n",
        "        if tl_boxes:\n",
        "            (x1,y1,x2,y2), _ = sorted(tl_boxes, key=lambda x:x[1], reverse=True)[0]\n",
        "            cv2.rectangle(draw, (x1,y1), (x2,y2), (0,255,255), 2)\n",
        "            cv2.putText(draw, f\"signal:{last_signal}\", (x1, max(20,y1-6)), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0,255,255), 2)\n",
        "        if train_roi is not None:\n",
        "            x1,y1,x2,y2 = train_roi\n",
        "            cv2.rectangle(draw, (x1,y1), (x2,y2), (0,128,255), 3)\n",
        "            cv2.putText(draw, f\"train:{ts.mode}\", (x1, min(H-6,y2+18)), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0,128,255), 2)\n",
        "        cv2.putText(draw, \"t=0.00s\", (10,30), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255,255,255), 2)\n",
        "        writer.write(draw)\n",
        "\n",
        "    # Iterate remaining frames\n",
        "    i = 1\n",
        "    while True:\n",
        "        ok, frame = cap.read()\n",
        "        if not ok:\n",
        "            break\n",
        "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "        t_s = i / float(fps)\n",
        "\n",
        "        tl_boxes, tr_boxes = det.infer(frame)\n",
        "        tl_roi = None\n",
        "        if tl_boxes:\n",
        "            (x1,y1,x2,y2), _ = sorted(tl_boxes, key=lambda x:x[1], reverse=True)[0]\n",
        "            x1,y1 = max(0,x1), max(0,y1)\n",
        "            x2,y2 = min(W-1,x2), min(H-1,y2)\n",
        "            if x2>x1 and y2>y1:\n",
        "                tl_roi = frame[y1:y2, x1:x2]\n",
        "        sig = classify_signal_color_bgr(tl_roi, prev_color=last_signal)\n",
        "\n",
        "        train_roi = pick_largest_valid_train(tr_boxes)\n",
        "        ts = update_train_state(ts, gray_prev, gray, train_roi)\n",
        "\n",
        "        changed = (sig != last_reported_signal) or (ts.mode != last_reported_train)\n",
        "        if changed:\n",
        "            print(format_event(t_s, sig, ts.mode))\n",
        "            last_reported_signal = sig\n",
        "            last_reported_train  = ts.mode\n",
        "\n",
        "        # Annotate if requested\n",
        "        if writer is not None:\n",
        "            draw = frame.copy()\n",
        "            if tl_boxes:\n",
        "                (x1,y1,x2,y2), _ = sorted(tl_boxes, key=lambda x:x[1], reverse=True)[0]\n",
        "                cv2.rectangle(draw, (x1,y1), (x2,y2), (0,255,255), 2)\n",
        "                cv2.putText(draw, f\"signal:{sig}\", (x1, max(20,y1-6)), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0,255,255), 2)\n",
        "            if train_roi is not None:\n",
        "                x1,y1,x2,y2 = train_roi\n",
        "                cv2.rectangle(draw, (x1,y1), (x2,y2), (0,128,255), 3)\n",
        "                cv2.putText(draw, f\"train:{ts.mode}\", (x1, min(H-6,y2+18)), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0,128,255), 2)\n",
        "            cv2.putText(draw, f\"t={t_s:.2f}s\", (10,30), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255,255,255), 2)\n",
        "            writer.write(draw)\n",
        "\n",
        "        gray_prev = gray\n",
        "        last_signal = sig\n",
        "        i += 1\n",
        "\n",
        "    cap.release()\n",
        "    if writer is not None:\n",
        "        writer.release()\n",
        "\n",
        "# ----------------------------\n",
        "# Optional diagnostic: XOR shift with provided images\n",
        "# ----------------------------\n",
        "def test_xor_dx_with_images(imgA=\"real_car_left_1.jpg\", imgB=\"real_car_left_2.jpg\"):\n",
        "    if not (os.path.exists(imgA) and os.path.exists(imgB)):\n",
        "        print(\"Sample images not found; skipping.\")\n",
        "        return\n",
        "    A = cv2.imread(imgA, cv2.IMREAD_GRAYSCALE)\n",
        "    B = cv2.imread(imgB, cv2.IMREAD_GRAYSCALE)\n",
        "    if A is None or B is None:\n",
        "        print(\"Unable to read test images.\")\n",
        "        return\n",
        "    dx = xor_dx(A, B, (0,0,A.shape[1],A.shape[0]))\n",
        "    direction = \"Right\" if dx > 0.5 else (\"Left\" if dx < -0.5 else \"None\")\n",
        "    print(f\"XOR dx ≈ {dx:.2f} px, direction = {direction}\")\n",
        "\n",
        "# ----------------------------\n",
        "# Entry point (set your path)\n",
        "# ----------------------------\n",
        "# Optional: handle Google Colab vs local\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    DRIVE_PATH = \"/content/drive/MyDrive/Deep Learning/Projects/Project2\"\n",
        "except ImportError:\n",
        "    print(\"Running outside Colab, skipping Google Drive mount\")\n",
        "    DRIVE_PATH = None\n",
        "\n",
        "\n",
        "VIDEO_PATH = \"/content/drive/MyDrive/videos/Input Video.mp4\"\n",
        "run_pipeline(VIDEO_PATH, write_video=cfg.annotate_video)\n",
        "# To test XOR fallback with the provided context images, optionally run:\n",
        "# test_xor_dx_with_images()\n",
        "# ============================================================\n",
        "# Notes:\n",
        "# - The output lines are printed only when either the signal color changes\n",
        "#   or the train state changes, with timestamps t computed from FPS, in the\n",
        "#   exact textual format required by Project2.pdf [Sections 3 & 6].\n",
        "# - Signal color is computed directly from BGR pixels in the YOLO traffic\n",
        "#   light ROI as permitted; no colorspace conversion is used.\n",
        "# - Train state transitions are determined by ORB feature motion within the\n",
        "#   YOLO train ROI, with a moving-edges XOR correlation fallback for low-texture\n",
        "#   frames, inspired by the provided class demos.\n",
        "# ============================================================\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0bb81bff"
      },
      "source": [
        "# Task\n",
        "Modify the existing code to detect the direction of the train and include this information in the output and the annotated video."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8b8fbb0"
      },
      "source": [
        "## Modify the `update train state` function\n",
        "\n",
        "### Subtask:\n",
        "Enhance the train state machine to incorporate direction detection based on the change in the train's bounding box over time or using the motion information from ORB features or the XOR fallback.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0e3e6ead"
      },
      "source": [
        "**Reasoning**:\n",
        "Modify the `TrainState` dataclass and the `update_train_state` function to include and update the train's direction based on calculated movement.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "da73eceb"
      },
      "source": [
        "from dataclasses import dataclass\n",
        "\n",
        "@dataclass\n",
        "class TrainState:\n",
        "    mode: str = \"not_present\"      # not_present | just_entered | moving | left_the_frame\n",
        "    direction: str = \"unknown\"     # unknown | left | right | stopped\n",
        "    miss_count: int = 0\n",
        "    just_counter: int = 0\n",
        "    had_presence: bool = False\n",
        "    last_roi: tuple = (0,0,0,0)\n",
        "    direction_history: list = None # For smoothing\n",
        "\n",
        "    def __post_init__(self):\n",
        "        if self.direction_history is None:\n",
        "            self.direction_history = []\n",
        "\n",
        "def update_train_state(ts: TrainState, prev_gray, curr_gray, roi):\n",
        "    present = roi is not None\n",
        "    current_direction = \"unknown\"\n",
        "\n",
        "    if not present:\n",
        "        ts.miss_count += 1\n",
        "        if ts.had_presence and ts.miss_count >= cfg.miss_patience and ts.mode != \"left_the_frame\":\n",
        "            ts.mode = \"left_the_frame\"\n",
        "            ts.had_presence = False\n",
        "            ts.just_counter = 0\n",
        "            current_direction = \"unknown\"\n",
        "        elif not ts.had_presence:\n",
        "            ts.mode = \"not_present\"\n",
        "            current_direction = \"unknown\"\n",
        "    else:\n",
        "        ts.miss_count = 0\n",
        "        if not ts.had_presence:\n",
        "            ts.mode = \"just_entered\"\n",
        "            ts.had_presence = True\n",
        "            ts.just_counter = 0\n",
        "            current_direction = \"unknown\"\n",
        "        else:\n",
        "            speed, nm = orb_speed(prev_gray, curr_gray, roi)\n",
        "            dx = xor_dx(prev_gray, curr_gray, roi)\n",
        "\n",
        "            if speed >= cfg.motion_speed_thresh:\n",
        "                if dx > 0.5: # Using a small threshold for significant movement\n",
        "                    current_direction = \"right\"\n",
        "                elif dx < -0.5:\n",
        "                    current_direction = \"left\"\n",
        "                else:\n",
        "                    current_direction = \"stopped\"\n",
        "            else:\n",
        "                # Maintain direction for a few frames if in moving state and speed is low\n",
        "                if ts.mode == \"moving\":\n",
        "                    if len(ts.direction_history) > 0:\n",
        "                        last_dir = ts.direction_history[-1]\n",
        "                        if last_dir in [\"left\", \"right\"]:\n",
        "                             current_direction = last_dir\n",
        "                        else:\n",
        "                            current_direction = \"stopped\"\n",
        "                    else:\n",
        "                        current_direction = \"stopped\" # Should not happen if in moving state\n",
        "                else:\n",
        "                    current_direction = \"stopped\"\n",
        "\n",
        "\n",
        "            if ts.mode == \"just_entered\":\n",
        "                ts.just_counter += 1\n",
        "                if ts.just_counter >= cfg.just_entered_grace or speed >= cfg.motion_speed_thresh:\n",
        "                    ts.mode = \"moving\"\n",
        "\n",
        "\n",
        "    # Smoothing the direction\n",
        "    ts.direction_history.append(current_direction)\n",
        "    # Keep only the last few directions (e.g., 5 frames) for smoothing\n",
        "    smoothing_window = 5\n",
        "    if len(ts.direction_history) > smoothing_window:\n",
        "        ts.direction_history = ts.direction_history[-smoothing_window:]\n",
        "\n",
        "    # Determine the final direction based on history (simple majority vote or last consistent direction)\n",
        "    # For simplicity, let's use the last consistent direction if available, otherwise the most frequent in history\n",
        "    if len(set(ts.direction_history)) == 1:\n",
        "         ts.direction = ts.direction_history[-1]\n",
        "    elif len(ts.direction_history) > 0 and ts.direction_history[-1] != \"unknown\":\n",
        "         ts.direction = ts.direction_history[-1] # Prioritize the most recent if not unknown\n",
        "    else:\n",
        "        # Simple majority vote\n",
        "        from collections import Counter\n",
        "        direction_counts = Counter(ts.direction_history)\n",
        "        most_common_direction = direction_counts.most_common(1)\n",
        "        if most_common_direction:\n",
        "            ts.direction = most_common_direction[0][0]\n",
        "        else:\n",
        "            ts.direction = \"unknown\"\n",
        "\n",
        "\n",
        "    ts.last_roi = roi\n",
        "    return ts\n",
        "\n",
        "def format_event(t_s, signal_color, train_mode, train_direction):\n",
        "    return f\"t = {t_s:.2f}s, signal = {signal_color}, train = {train_mode}, direction = {train_direction}\"\n",
        "\n",
        "# Modify the main pipeline to use the updated format_event and TrainState\n",
        "def run_pipeline(video_path, write_video=cfg.annotate_video):\n",
        "    det = Detector(cfg.yolo_weights)\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    if not cap.isOpened():\n",
        "        raise SystemExit(f\"Cannot open video: {video_path}\")\n",
        "\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS) or 30.0\n",
        "    W  = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH) or 640)\n",
        "    H  = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT) or 360)\n",
        "\n",
        "    writer = None\n",
        "    if write_video:\n",
        "        fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
        "        writer = cv2.VideoWriter(cfg.out_video_path, fourcc, fps, (W, H))\n",
        "\n",
        "    ok, frame0 = cap.read()\n",
        "    if not ok:\n",
        "        raise SystemExit(\"Empty video.\")\n",
        "    gray_prev = cv2.cvtColor(frame0, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Process first frame at t=0\n",
        "    tl_boxes, tr_boxes = det.infer(frame0)\n",
        "    tl_roi = None\n",
        "    if tl_boxes:\n",
        "        (x1,y1,x2,y2), _ = sorted(tl_boxes, key=lambda x:x[1], reverse=True)[0]\n",
        "        x1,y1 = max(0,x1), max(0,y1)\n",
        "        x2,y2 = min(W-1,x2), min(H-1,y2)\n",
        "        if x2>x1 and y2>y1:\n",
        "            tl_roi = frame0[y1:y2, x1:x2]\n",
        "    last_signal = classify_signal_color_bgr(tl_roi, prev_color=None)\n",
        "    ts = TrainState()\n",
        "    train_roi = pick_largest_valid_train(tr_boxes)\n",
        "    ts = update_train_state(ts, gray_prev, gray_prev, train_roi)  # prev==curr at t=0\n",
        "    last_reported_signal = None\n",
        "    last_reported_train_mode  = None\n",
        "    last_reported_train_direction = None\n",
        "\n",
        "\n",
        "    # Emit initial line\n",
        "    line0 = format_event(0.0, last_signal, ts.mode, ts.direction)\n",
        "    print(line0)\n",
        "    last_reported_signal = last_signal\n",
        "    last_reported_train_mode  = ts.mode\n",
        "    last_reported_train_direction = ts.direction\n",
        "\n",
        "    # Annotate first frame\n",
        "    if writer is not None:\n",
        "        draw = frame0.copy()\n",
        "        if tl_boxes:\n",
        "            (x1,y1,x2,y2), _ = sorted(tl_boxes, key=lambda x:x[1], reverse=True)[0]\n",
        "            cv2.rectangle(draw, (x1,y1), (x2,y2), (0,255,255), 2)\n",
        "            cv2.putText(draw, f\"signal:{last_signal}\", (x1, max(20,y1-6)), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0,255,255), 2)\n",
        "        if train_roi is not None:\n",
        "            x1,y1,x2,y2 = train_roi\n",
        "            cv2.rectangle(draw, (x1,y1), (x2,y2), (0,128,255), 3)\n",
        "            cv2.putText(draw, f\"train:{ts.mode} ({ts.direction})\", (x1, min(H-6,y2+18)), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0,128,255), 2)\n",
        "        cv2.putText(draw, f\"t=0.00s\", (10,30), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255,255,255), 2)\n",
        "        writer.write(draw)\n",
        "\n",
        "    # Iterate remaining frames\n",
        "    i = 1\n",
        "    while True:\n",
        "        ok, frame = cap.read()\n",
        "        if not ok:\n",
        "            break\n",
        "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "        t_s = i / float(fps)\n",
        "\n",
        "        tl_boxes, tr_boxes = det.infer(frame)\n",
        "        tl_roi = None\n",
        "        if tl_boxes:\n",
        "            (x1,y1,x2,y2), _ = sorted(tl_boxes, key=lambda x:x[1], reverse=True)[0]\n",
        "            x1,y1 = max(0,x1), max(0,y1)\n",
        "            x2,y2 = min(W-1,x2), min(H-1,y2)\n",
        "            if x2>x1 and y2>y1:\n",
        "                tl_roi = frame[y1:y2, x1:x2]\n",
        "        sig = classify_signal_color_bgr(tl_roi, prev_color=last_signal)\n",
        "\n",
        "        train_roi = pick_largest_valid_train(tr_boxes)\n",
        "        ts = update_train_state(ts, gray_prev, gray, train_roi)\n",
        "\n",
        "        changed = (sig != last_reported_signal) or (ts.mode != last_reported_train_mode) or (ts.direction != last_reported_train_direction)\n",
        "        if changed:\n",
        "            print(format_event(t_s, sig, ts.mode, ts.direction))\n",
        "            last_reported_signal = sig\n",
        "            last_reported_train_mode  = ts.mode\n",
        "            last_reported_train_direction = ts.direction\n",
        "\n",
        "        # Annotate if requested\n",
        "        if writer is not None:\n",
        "            draw = frame.copy()\n",
        "            if tl_boxes:\n",
        "                (x1,y1,x2,y2), _ = sorted(tl_boxes, key=lambda x:x[1], reverse=True)[0]\n",
        "                cv2.rectangle(draw, (x1,y1), (x2,y2), (0,255,255), 2)\n",
        "                cv2.putText(draw, f\"signal:{sig}\", (x1, max(20,y1-6)), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0,255,255), 2)\n",
        "            if train_roi is not None:\n",
        "                x1,y1,x2,y2 = train_roi\n",
        "                cv2.rectangle(draw, (x1,y1), (x2,y2), (0,128,255), 3)\n",
        "                cv2.putText(draw, f\"train:{ts.mode} ({ts.direction})\", (x1, min(H-6,y2+18)), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0,128,255), 2)\n",
        "            cv2.putText(draw, f\"t={t_s:.2f}s\", (10,30), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255,255,255), 2)\n",
        "            writer.write(draw)\n",
        "\n",
        "        gray_prev = gray\n",
        "        last_signal = sig\n",
        "        i += 1\n",
        "\n",
        "    cap.release()\n",
        "    if writer is not None:\n",
        "        writer.release()\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dc9b8906"
      },
      "source": [
        "## Update the `trainstate` dataclass\n",
        "\n",
        "### Subtask:\n",
        "Add a field to store the detected train direction (e.g., \"left\", \"right\", \"stopped\", \"unknown\").\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4c20709c"
      },
      "source": [
        "## Modify the output format\n",
        "\n",
        "### Subtask:\n",
        "Modify the printed output lines to include the detected train direction when the train state changes.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d842e735"
      },
      "source": [
        "**Reasoning**:\n",
        "Modify the print statement in the run_pipeline function to include the train direction, as per the instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d8df068e",
        "outputId": "2f547848-bcf9-48b8-ac11-7f503b0bc21e"
      },
      "source": [
        "def run_pipeline(video_path, write_video=cfg.annotate_video):\n",
        "    det = Detector(cfg.yolo_weights)\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    if not cap.isOpened():\n",
        "        raise SystemExit(f\"Cannot open video: {video_path}\")\n",
        "\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS) or 30.0\n",
        "    W  = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH) or 640)\n",
        "    H  = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT) or 360)\n",
        "\n",
        "    writer = None\n",
        "    if write_video:\n",
        "        fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
        "        writer = cv2.VideoWriter(cfg.out_video_path, fourcc, fps, (W, H))\n",
        "\n",
        "    ok, frame0 = cap.read()\n",
        "    if not ok:\n",
        "        raise SystemExit(\"Empty video.\")\n",
        "    gray_prev = cv2.cvtColor(frame0, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Process first frame at t=0\n",
        "    tl_boxes, tr_boxes = det.infer(frame0)\n",
        "    tl_roi = None\n",
        "    if tl_boxes:\n",
        "        (x1,y1,x2,y2), _ = sorted(tl_boxes, key=lambda x:x[1], reverse=True)[0]\n",
        "        x1,y1 = max(0,x1), max(0,y1)\n",
        "        x2,y2 = min(W-1,x2), min(H-1,y2)\n",
        "        if x2>x1 and y2>y1:\n",
        "            tl_roi = frame0[y1:y2, x1:x2]\n",
        "    last_signal = classify_signal_color_bgr(tl_roi, prev_color=None)\n",
        "    ts = TrainState()\n",
        "    train_roi = pick_largest_valid_train(tr_boxes)\n",
        "    ts = update_train_state(ts, gray_prev, gray_prev, train_roi)  # prev==curr at t=0\n",
        "    last_reported_signal = None\n",
        "    last_reported_train_mode  = None\n",
        "    last_reported_train_direction = None\n",
        "\n",
        "    # Emit initial line\n",
        "    line0 = format_event(0.0, last_signal, ts.mode, ts.direction)\n",
        "    print(line0)\n",
        "    last_reported_signal = last_signal\n",
        "    last_reported_train_mode  = ts.mode\n",
        "    last_reported_train_direction = ts.direction\n",
        "\n",
        "    # Annotate first frame\n",
        "    if writer is not None:\n",
        "        draw = frame0.copy()\n",
        "        if tl_boxes:\n",
        "            (x1,y1,x2,y2), _ = sorted(tl_boxes, key=lambda x:x[1], reverse=True)[0]\n",
        "            cv2.rectangle(draw, (x1,y1), (x2,y2), (0,255,255), 2)\n",
        "            cv2.putText(draw, f\"signal:{last_signal}\", (x1, max(20,y1-6)), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0,255,255), 2)\n",
        "        if train_roi is not None:\n",
        "            x1,y1,x2,y2 = train_roi\n",
        "            cv2.rectangle(draw, (x1,y1), (x2,y2), (0,128,255), 3)\n",
        "            cv2.putText(draw, f\"train:{ts.mode} ({ts.direction})\", (x1, min(H-6,y2+18)), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0,128,255), 2)\n",
        "        cv2.putText(draw, f\"t=0.00s\", (10,30), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255,255,255), 2)\n",
        "        writer.write(draw)\n",
        "\n",
        "    # Iterate remaining frames\n",
        "    i = 1\n",
        "    while True:\n",
        "        ok, frame = cap.read()\n",
        "        if not ok:\n",
        "            break\n",
        "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "        t_s = i / float(fps)\n",
        "\n",
        "        tl_boxes, tr_boxes = det.infer(frame)\n",
        "        tl_roi = None\n",
        "        if tl_boxes:\n",
        "            (x1,y1,x2,y2), _ = sorted(tl_boxes, key=lambda x:x[1], reverse=True)[0]\n",
        "            x1,y1 = max(0,x1), max(0,y1)\n",
        "            x2,y2 = min(W-1,x2), min(H-1,y2)\n",
        "            if x2>x1 and y2>y1:\n",
        "                tl_roi = frame[y1:y2, x1:x2]\n",
        "        sig = classify_signal_color_bgr(tl_roi, prev_color=last_signal)\n",
        "\n",
        "        train_roi = pick_largest_valid_train(tr_boxes)\n",
        "        ts = update_train_state(ts, gray_prev, gray, train_roi)\n",
        "\n",
        "        changed = (sig != last_reported_signal) or (ts.mode != last_reported_train_mode) or (ts.direction != last_reported_train_direction)\n",
        "        if changed:\n",
        "            print(format_event(t_s, sig, ts.mode, ts.direction)) # Modified print statement\n",
        "            last_reported_signal = sig\n",
        "            last_reported_train_mode  = ts.mode\n",
        "            last_reported_train_direction = ts.direction\n",
        "\n",
        "        # Annotate if requested\n",
        "        if writer is not None:\n",
        "            draw = frame.copy()\n",
        "            if tl_boxes:\n",
        "                (x1,y1,x2,y2), _ = sorted(tl_boxes, key=lambda x:x[1], reverse=True)[0]\n",
        "                cv2.rectangle(draw, (x1,y1), (x2,y2), (0,255,255), 2)\n",
        "                cv2.putText(draw, f\"signal:{sig}\", (x1, max(20,y1-6)), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0,255,255), 2)\n",
        "            if train_roi is not None:\n",
        "                x1,y1,x2,y2 = train_roi\n",
        "                cv2.rectangle(draw, (x1,y1), (x2,y2), (0,128,255), 3)\n",
        "                cv2.putText(draw, f\"train:{ts.mode} ({ts.direction})\", (x1, min(H-6,y2+18)), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0,128,255), 2)\n",
        "            cv2.putText(draw, f\"t={t_s:.2f}s\", (10,30), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255,255,255), 2)\n",
        "            writer.write(draw)\n",
        "\n",
        "        gray_prev = gray\n",
        "        last_signal = sig\n",
        "        i += 1\n",
        "\n",
        "    cap.release()\n",
        "    if writer is not None:\n",
        "        writer.release()\n",
        "\n",
        "# Rerun the pipeline with the modified function\n",
        "VIDEO_PATH = \"/content/drive/MyDrive/videos/Input Video.mp4\"\n",
        "run_pipeline(VIDEO_PATH, write_video=cfg.annotate_video)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "t = 0.00s, signal = red, train = not_present, direction = unknown\n",
            "t = 1.90s, signal = red, train = just_entered, direction = unknown\n",
            "t = 1.95s, signal = red, train = moving, direction = left\n",
            "t = 1.97s, signal = red, train = moving, direction = unknown\n",
            "t = 2.00s, signal = red, train = moving, direction = left\n",
            "t = 2.02s, signal = red, train = moving, direction = unknown\n",
            "t = 2.07s, signal = red, train = moving, direction = left\n",
            "t = 3.67s, signal = red, train = moving, direction = right\n",
            "t = 3.68s, signal = red, train = moving, direction = left\n",
            "t = 3.72s, signal = red, train = moving, direction = right\n",
            "t = 3.73s, signal = red, train = moving, direction = left\n",
            "t = 3.75s, signal = red, train = moving, direction = right\n",
            "t = 3.78s, signal = red, train = moving, direction = left\n",
            "t = 3.80s, signal = red, train = moving, direction = right\n",
            "t = 3.83s, signal = red, train = moving, direction = left\n",
            "t = 3.87s, signal = red, train = moving, direction = right\n",
            "t = 3.88s, signal = red, train = moving, direction = left\n",
            "t = 3.95s, signal = red, train = moving, direction = right\n",
            "t = 3.98s, signal = red, train = moving, direction = left\n",
            "t = 4.00s, signal = red, train = moving, direction = right\n",
            "t = 4.03s, signal = red, train = moving, direction = left\n",
            "t = 4.05s, signal = red, train = moving, direction = right\n",
            "t = 4.08s, signal = red, train = moving, direction = left\n",
            "t = 4.10s, signal = red, train = moving, direction = right\n",
            "t = 4.12s, signal = red, train = moving, direction = left\n",
            "t = 4.15s, signal = red, train = moving, direction = right\n",
            "t = 4.18s, signal = red, train = moving, direction = left\n",
            "t = 4.30s, signal = red, train = moving, direction = right\n",
            "t = 4.32s, signal = red, train = moving, direction = left\n",
            "t = 4.40s, signal = red, train = moving, direction = right\n",
            "t = 4.42s, signal = red, train = moving, direction = left\n",
            "t = 4.62s, signal = red, train = moving, direction = right\n",
            "t = 4.63s, signal = red, train = moving, direction = left\n",
            "t = 4.65s, signal = red, train = moving, direction = right\n",
            "t = 4.67s, signal = red, train = moving, direction = left\n",
            "t = 4.70s, signal = red, train = moving, direction = right\n",
            "t = 4.73s, signal = red, train = moving, direction = left\n",
            "t = 4.92s, signal = red, train = moving, direction = right\n",
            "t = 4.93s, signal = red, train = moving, direction = left\n",
            "t = 4.97s, signal = red, train = moving, direction = right\n",
            "t = 5.02s, signal = red, train = moving, direction = left\n",
            "t = 5.05s, signal = red, train = moving, direction = right\n",
            "t = 5.08s, signal = red, train = moving, direction = left\n",
            "t = 5.12s, signal = red, train = moving, direction = right\n",
            "t = 5.13s, signal = red, train = moving, direction = left\n",
            "t = 5.15s, signal = red, train = moving, direction = right\n",
            "t = 5.18s, signal = red, train = moving, direction = left\n",
            "t = 5.22s, signal = red, train = moving, direction = right\n",
            "t = 5.23s, signal = red, train = moving, direction = left\n",
            "t = 5.27s, signal = red, train = moving, direction = right\n",
            "t = 5.28s, signal = red, train = moving, direction = left\n",
            "t = 5.30s, signal = red, train = moving, direction = right\n",
            "t = 5.32s, signal = red, train = moving, direction = left\n",
            "t = 5.37s, signal = red, train = moving, direction = right\n",
            "t = 5.38s, signal = red, train = moving, direction = left\n",
            "t = 5.40s, signal = red, train = moving, direction = right\n",
            "t = 5.43s, signal = red, train = moving, direction = left\n",
            "t = 5.47s, signal = red, train = moving, direction = right\n",
            "t = 5.48s, signal = red, train = moving, direction = left\n",
            "t = 5.50s, signal = red, train = moving, direction = right\n",
            "t = 5.52s, signal = red, train = moving, direction = left\n",
            "t = 5.85s, signal = red, train = moving, direction = right\n",
            "t = 5.88s, signal = red, train = moving, direction = left\n",
            "t = 5.90s, signal = red, train = moving, direction = right\n",
            "t = 5.95s, signal = red, train = moving, direction = unknown\n",
            "t = 6.00s, signal = red, train = left_the_frame, direction = unknown\n",
            "t = 6.02s, signal = red, train = not_present, direction = unknown\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fd95a33c"
      },
      "source": [
        "## Modify the annotation\n",
        "\n",
        "### Subtask:\n",
        "Update the video annotation to display the detected train direction along with the train state.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3e1e434d"
      },
      "source": [
        "**Reasoning**:\n",
        "Modify the cv2.putText calls in the run_pipeline function to include the train direction in the annotation text for both the initial frame and the main loop.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d1ffeb5a",
        "outputId": "3fd728bc-3e0f-49c9-cdc1-cd96e9bdac3a"
      },
      "source": [
        "# Modify the main pipeline to use the updated format_event and TrainState\n",
        "def run_pipeline(video_path, write_video=cfg.annotate_video):\n",
        "    det = Detector(cfg.yolo_weights)\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    if not cap.isOpened():\n",
        "        raise SystemExit(f\"Cannot open video: {video_path}\")\n",
        "\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS) or 30.0\n",
        "    W  = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH) or 640)\n",
        "    H  = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT) or 360)\n",
        "\n",
        "    writer = None\n",
        "    if write_video:\n",
        "        fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
        "        writer = cv2.VideoWriter(cfg.out_video_path, fourcc, fps, (W, H))\n",
        "\n",
        "    ok, frame0 = cap.read()\n",
        "    if not ok:\n",
        "        raise SystemExit(\"Empty video.\")\n",
        "    gray_prev = cv2.cvtColor(frame0, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Process first frame at t=0\n",
        "    tl_boxes, tr_boxes = det.infer(frame0)\n",
        "    tl_roi = None\n",
        "    if tl_boxes:\n",
        "        (x1,y1,x2,y2), _ = sorted(tl_boxes, key=lambda x:x[1], reverse=True)[0]\n",
        "        x1,y1 = max(0,x1), max(0,y1)\n",
        "        x2,y2 = min(W-1,x2), min(H-1,y2)\n",
        "        if x2>x1 and y2>y1:\n",
        "            tl_roi = frame0[y1:y2, x1:x2]\n",
        "    last_signal = classify_signal_color_bgr(tl_roi, prev_color=None)\n",
        "    ts = TrainState()\n",
        "    train_roi = pick_largest_valid_train(tr_boxes)\n",
        "    ts = update_train_state(ts, gray_prev, gray_prev, train_roi)  # prev==curr at t=0\n",
        "    last_reported_signal = None\n",
        "    last_reported_train_mode  = None\n",
        "    last_reported_train_direction = None\n",
        "\n",
        "\n",
        "    # Emit initial line\n",
        "    line0 = format_event(0.0, last_signal, ts.mode, ts.direction)\n",
        "    print(line0)\n",
        "    last_reported_signal = last_signal\n",
        "    last_reported_train_mode  = ts.mode\n",
        "    last_reported_train_direction = ts.direction\n",
        "\n",
        "    # Annotate first frame\n",
        "    if writer is not None:\n",
        "        draw = frame0.copy()\n",
        "        if tl_boxes:\n",
        "            (x1,y1,x2,y2), _ = sorted(tl_boxes, key=lambda x:x[1], reverse=True)[0]\n",
        "            cv2.rectangle(draw, (x1,y1), (x2,y2), (0,255,255), 2)\n",
        "            cv2.putText(draw, f\"signal:{last_signal}\", (x1, max(20,y1-6)), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0,255,255), 2)\n",
        "        if train_roi is not None:\n",
        "            x1,y1,x2,y2 = train_roi\n",
        "            cv2.rectangle(draw, (x1,y1), (x2,y2), (0,128,255), 3)\n",
        "            cv2.putText(draw, f\"train:{ts.mode} ({ts.direction})\", (x1, min(H-6,y2+18)), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0,128,255), 2) # Modified annotation\n",
        "        cv2.putText(draw, f\"t=0.00s\", (10,30), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255,255,255), 2)\n",
        "        writer.write(draw)\n",
        "\n",
        "    # Iterate remaining frames\n",
        "    i = 1\n",
        "    while True:\n",
        "        ok, frame = cap.read()\n",
        "        if not ok:\n",
        "            break\n",
        "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "        t_s = i / float(fps)\n",
        "\n",
        "        tl_boxes, tr_boxes = det.infer(frame)\n",
        "        tl_roi = None\n",
        "        if tl_boxes:\n",
        "            (x1,y1,x2,y2), _ = sorted(tl_boxes, key=lambda x:x[1], reverse=True)[0]\n",
        "            x1,y1 = max(0,x1), max(0,y1)\n",
        "            x2,y2 = min(W-1,x2), min(H-1,y2)\n",
        "            if x2>x1 and y2>y1:\n",
        "                tl_roi = frame[y1:y2, x1:x2]\n",
        "        sig = classify_signal_color_bgr(tl_roi, prev_color=last_signal)\n",
        "\n",
        "        train_roi = pick_largest_valid_train(tr_boxes)\n",
        "        ts = update_train_state(ts, gray_prev, gray, train_roi)\n",
        "\n",
        "        changed = (sig != last_reported_signal) or (ts.mode != last_reported_train_mode) or (ts.direction != last_reported_train_direction)\n",
        "        if changed:\n",
        "            print(format_event(t_s, sig, ts.mode, ts.direction))\n",
        "            last_reported_signal = sig\n",
        "            last_reported_train_mode  = ts.mode\n",
        "            last_reported_train_direction = ts.direction\n",
        "\n",
        "        # Annotate if requested\n",
        "        if writer is not None:\n",
        "            draw = frame.copy()\n",
        "            if tl_boxes:\n",
        "                (x1,y1,x2,y2), _ = sorted(tl_boxes, key=lambda x:x[1], reverse=True)[0]\n",
        "                cv2.rectangle(draw, (x1,y1), (x2,y2), (0,255,255), 2)\n",
        "                cv2.putText(draw, f\"signal:{sig}\", (x1, max(20,y1-6)), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0,255,255), 2)\n",
        "            if train_roi is not None:\n",
        "                x1,y1,x2,y2 = train_roi\n",
        "                cv2.rectangle(draw, (x1,y1), (x2,y2), (0,128,255), 3)\n",
        "                cv2.putText(draw, f\"train:{ts.mode} ({ts.direction})\", (x1, min(H-6,y2+18)), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0,128,255), 2) # Modified annotation\n",
        "            cv2.putText(draw, f\"t={t_s:.2f}s\", (10,30), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255,255,255), 2)\n",
        "            writer.write(draw)\n",
        "\n",
        "        gray_prev = gray\n",
        "        last_signal = sig\n",
        "        i += 1\n",
        "\n",
        "    cap.release()\n",
        "    if writer is not None:\n",
        "        writer.release()\n",
        "\n",
        "# Rerun the pipeline with the modified function\n",
        "VIDEO_PATH = \"/content/drive/MyDrive/videos/Input Video.mp4\"\n",
        "run_pipeline(VIDEO_PATH, write_video=cfg.annotate_video)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "t = 0.00s, signal = red, train = not_present, direction = unknown\n",
            "t = 1.90s, signal = red, train = just_entered, direction = unknown\n",
            "t = 1.95s, signal = red, train = moving, direction = left\n",
            "t = 1.97s, signal = red, train = moving, direction = unknown\n",
            "t = 2.00s, signal = red, train = moving, direction = left\n",
            "t = 2.02s, signal = red, train = moving, direction = unknown\n",
            "t = 2.07s, signal = red, train = moving, direction = left\n",
            "t = 3.67s, signal = red, train = moving, direction = right\n",
            "t = 3.68s, signal = red, train = moving, direction = left\n",
            "t = 3.72s, signal = red, train = moving, direction = right\n",
            "t = 3.73s, signal = red, train = moving, direction = left\n",
            "t = 3.75s, signal = red, train = moving, direction = right\n",
            "t = 3.78s, signal = red, train = moving, direction = left\n",
            "t = 3.80s, signal = red, train = moving, direction = right\n",
            "t = 3.83s, signal = red, train = moving, direction = left\n",
            "t = 3.87s, signal = red, train = moving, direction = right\n",
            "t = 3.88s, signal = red, train = moving, direction = left\n",
            "t = 3.95s, signal = red, train = moving, direction = right\n",
            "t = 3.98s, signal = red, train = moving, direction = left\n",
            "t = 4.00s, signal = red, train = moving, direction = right\n",
            "t = 4.03s, signal = red, train = moving, direction = left\n",
            "t = 4.05s, signal = red, train = moving, direction = right\n",
            "t = 4.08s, signal = red, train = moving, direction = left\n",
            "t = 4.10s, signal = red, train = moving, direction = right\n",
            "t = 4.12s, signal = red, train = moving, direction = left\n",
            "t = 4.15s, signal = red, train = moving, direction = right\n",
            "t = 4.18s, signal = red, train = moving, direction = left\n",
            "t = 4.30s, signal = red, train = moving, direction = right\n",
            "t = 4.32s, signal = red, train = moving, direction = left\n",
            "t = 4.40s, signal = red, train = moving, direction = right\n",
            "t = 4.42s, signal = red, train = moving, direction = left\n",
            "t = 4.62s, signal = red, train = moving, direction = right\n",
            "t = 4.63s, signal = red, train = moving, direction = left\n",
            "t = 4.65s, signal = red, train = moving, direction = right\n",
            "t = 4.67s, signal = red, train = moving, direction = left\n",
            "t = 4.70s, signal = red, train = moving, direction = right\n",
            "t = 4.73s, signal = red, train = moving, direction = left\n",
            "t = 4.92s, signal = red, train = moving, direction = right\n",
            "t = 4.93s, signal = red, train = moving, direction = left\n",
            "t = 4.97s, signal = red, train = moving, direction = right\n",
            "t = 5.02s, signal = red, train = moving, direction = left\n",
            "t = 5.05s, signal = red, train = moving, direction = right\n",
            "t = 5.08s, signal = red, train = moving, direction = left\n",
            "t = 5.12s, signal = red, train = moving, direction = right\n",
            "t = 5.13s, signal = red, train = moving, direction = left\n",
            "t = 5.15s, signal = red, train = moving, direction = right\n",
            "t = 5.18s, signal = red, train = moving, direction = left\n",
            "t = 5.22s, signal = red, train = moving, direction = right\n",
            "t = 5.23s, signal = red, train = moving, direction = left\n",
            "t = 5.27s, signal = red, train = moving, direction = right\n",
            "t = 5.28s, signal = red, train = moving, direction = left\n",
            "t = 5.30s, signal = red, train = moving, direction = right\n",
            "t = 5.32s, signal = red, train = moving, direction = left\n",
            "t = 5.37s, signal = red, train = moving, direction = right\n",
            "t = 5.38s, signal = red, train = moving, direction = left\n",
            "t = 5.40s, signal = red, train = moving, direction = right\n",
            "t = 5.43s, signal = red, train = moving, direction = left\n",
            "t = 5.47s, signal = red, train = moving, direction = right\n",
            "t = 5.48s, signal = red, train = moving, direction = left\n",
            "t = 5.50s, signal = red, train = moving, direction = right\n",
            "t = 5.52s, signal = red, train = moving, direction = left\n",
            "t = 5.85s, signal = red, train = moving, direction = right\n",
            "t = 5.88s, signal = red, train = moving, direction = left\n",
            "t = 5.90s, signal = red, train = moving, direction = right\n",
            "t = 5.95s, signal = red, train = moving, direction = unknown\n",
            "t = 6.00s, signal = red, train = left_the_frame, direction = unknown\n",
            "t = 6.02s, signal = red, train = not_present, direction = unknown\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1518e64"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   The `TrainState` dataclass was successfully updated to include a `direction` attribute to store the detected train direction.\n",
        "*   The `update_train_state` function was enhanced to detect the train's direction (\"left\", \"right\", \"stopped\", or \"unknown\") based on motion analysis (ORB features and XOR fallback) and speed thresholds. A smoothing mechanism was implemented using a direction history list to improve the robustness of the direction detection.\n",
        "*   The `format_event` function and the main `run_pipeline` function were modified to include the detected train direction in the printed output logs. The output now includes the direction when the train state changes.\n",
        "*   The video annotation in the `run_pipeline` function was updated to display the detected train direction alongside the train state in the format \"train:{ts.mode} ({ts.direction})\".\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   Further refinement of the direction smoothing algorithm could be explored to handle edge cases or rapid changes in direction more effectively.\n",
        "*   Consider evaluating the performance of the direction detection using different motion analysis techniques or larger smoothing windows to optimize accuracy and responsiveness.\n"
      ]
    }
  ]
}